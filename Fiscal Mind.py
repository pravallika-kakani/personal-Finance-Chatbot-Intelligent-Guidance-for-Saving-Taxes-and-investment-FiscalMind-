# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GWYZ7o_sfBkkjeI8VCLt02mCEeyhOwSk
"""

!pip install -q transformers accelerate safetensors huggingface-hub

# ============================================================
#  FicalMind ‚Äì Stable Single-Cell Version (HuggingFace API Only)
# ============================================================

import os
import gradio as gr
from huggingface_hub import InferenceClient
from transformers import MarianMTModel, MarianTokenizer, pipeline

# ---------------------------------------------
# CONFIG
# ---------------------------------------------
MODEL_ID = "ibm-granite/granite-3.3-2b-instruct"
HF_TOKEN = os.environ.get("HUGGINGFACEHUB_API_TOKEN")

if HF_TOKEN is None:
    raise ValueError("Please set HUGGINGFACEHUB_API_TOKEN environment variable.")

hf_client = InferenceClient(token=HF_TOKEN)

# Indian language translation support
TRANSLATION_MODELS = {
    ("en","hi"): "Helsinki-NLP/opus-mt-en-hi",
    ("hi","en"): "Helsinki-NLP/opus-mt-hi-en"
}

translation_cache = {}

def translate(text, src, tgt):
    if src == tgt:
        return text
    key = (src, tgt)
    if key not in TRANSLATION_MODELS:
        return text

    model_name = TRANSLATION_MODELS[key]
    if model_name not in translation_cache:
        tok = MarianTokenizer.from_pretrained(model_name)
        mod = MarianMTModel.from_pretrained(model_name)
        translation_cache[model_name] = (tok, mod)
    tok, mod = translation_cache[model_name]

    batch = tok.prepare_seq2seq_batch([text], return_tensors="pt")
    gen = mod.generate(**batch)
    out = tok.batch_decode(gen, skip_special_tokens=True)[0]
    return out

# Speech recognition (Whisper small)
try:
    asr = pipeline("automatic-speech-recognition", model="openai/whisper-small")
except:
    asr = None

# -------------------------------------------------------
# Core financial guidance generator using Granite 3.3 2B
# -------------------------------------------------------
def generate_ficalmind_reply(user_text, lang, user_type, history):

    # Map tone
    if user_type == "student":
        tone = ("Use simple language, friendly tone, short sentences, "
                "explain basics of savings, taxes, investments, budgeting.")
    else:
        tone = ("Use a professional, formal, concise financial-advisor tone "
                "with clear structured insights.")

    # Persona instruction
    system_instruction = f"""
You are **FicalMind**, a formal and professional multimodal financial advisor chatbot.
You provide:
- Personalized guidance on savings, taxes, investments, and budgeting
- AI-generated budget summaries
- Spending insights and optimization advice
- Demographic-aware communication ({user_type} mode)
- Clear, safe, non-risky financial suggestions
- Translate answers into {lang} if needed

Tone rule:
{tone}

If user asks in Hindi, reply in Hindi. If user selects Hindi output, translate reply to Hindi.
"""

    # Build conversation
    conversation = system_instruction + "\n\n"
    for u, a in history:
        conversation += f"User: {u}\nAssistant: {a}\n"
    conversation += f"User: {user_text}\nAssistant:"

    # Generate via HF Inference API
    response = hf_client.text_generation(
        model=MODEL_ID,
        inputs=conversation,
        parameters={
            "max_new_tokens": 250,
            "temperature": 0.2,
            "top_p": 0.95
        }
    )

    reply = response
    return reply


# -------------------------------------------------------
# Gradio Chat Logic
# -------------------------------------------------------
def chat_logic(audio, text, lang, user_type, history):

    # Step 1: Get user input (audio > text)
    if audio is not None and asr is not None:
        trans = asr(audio)["text"]
        user_raw = trans
    else:
        user_raw = text

    if user_raw is None or user_raw.strip() == "":
        return history, ""

    # Step 2: Translate user input to English for AI model
    user_en = translate(user_raw, lang, "en")

    # Step 3: Generate reply
    reply_en = generate_ficalmind

!pip install -q gradio transformers huggingface-hub accelerate sentencepiece

import gradio as gr
from huggingface_hub import InferenceClient

# --------------------------------------
#  ENTER YOUR HUGGINGFACE TOKEN HERE
# --------------------------------------
HF_TOKEN = "PASTE_YOUR_TOKEN_HERE"
# --------------------------------------

if HF_TOKEN.strip() == "":
    raise ValueError("‚ùó Please paste your HuggingFace token in HF_TOKEN.")

# LLM Client (Chat Model)
llm = InferenceClient(
    "meta-llama/Llama-3.2-3B-Instruct",
    token=HF_TOKEN
)

# Whisper STT (Speech ‚Üí Text)
whisper = InferenceClient(
    "openai/whisper-large-v3",
    token=HF_TOKEN
)

# Text-to-Speech (FastSpeech2)
tts = InferenceClient(
    "facebook/fastspeech2-en-ljspeech",
    token=HF_TOKEN
)


# -------------------------
# LLM TEXT CHAT FUNCTION
# -------------------------
def chat_with_llm(message, history):
    messages = [{"role": "system", "content": "You are a helpful AI assistant."}]

    for user, bot in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": bot})

    messages.append({"role": "user", "content": message})

    response = llm.chat.completions.create(
        model="meta-llama/Llama-3.2-3B-Instruct",
        messages=messages,
        max_tokens=500,
    )

    return response.choices[0].message["content"]


# -------------------------
# SPEECH TO TEXT
# -------------------------
def transcribe(audio):
    if audio is None:
        return ""

    result = whisper.audio.transcriptions.create(
        model="openai/whisper-large-v3",
        file=audio
    )
    return result.text


# -------------------------
# TEXT TO SPEECH
# -------------------------
def speak(text):
    speech = tts.audio.speech.create(
        model="facebook/fastspeech2-en-ljspeech",
        input=text
    )
    output_path = "/content/voice_reply.wav"
    with open(output_path, "wb") as f:
        f.write(speech)
    return output_path


# -------------------------
# GRADIO APP
# -------------------------
with gr.Blocks() as demo:
    gr.Markdown("## üé§ AI Voice + Text Chat Assistant")

    chatbot = gr.Chatbot(height=350)

    with gr.Row():
        text_input = gr.Textbox(label="Type your message")
        send_btn = gr.Button("Send")

    with gr.Row():

!pip install -q gradio transformers huggingface-hub accelerate sentencepiece

import gradio as gr
from huggingface_hub import InferenceClient

# --------------------------------------
# ENTER YOUR HUGGINGFACE TOKEN HERE
# --------------------------------------
HF_TOKEN = "PASTE_YOUR_TOKEN_HERE"
# --------------------------------------

if HF_TOKEN.strip() == "":
    raise ValueError("‚ùó Please paste your HuggingFace token in HF_TOKEN.")

# LLM Client (Chat Model)
llm = InferenceClient(
    "meta-llama/Llama-3.2-3B-Instruct",
    token=HF_TOKEN
)

# Whisper STT (Speech-to-Text)
whisper = InferenceClient(
    "openai/whisper-large-v3",
    token=HF_TOKEN
)

# Text-to-Speech Model
tts = InferenceClient(
    "facebook/fastspeech2-en-ljspeech",
    token=HF_TOKEN
)

# -------------------------
# LLM TEXT CHAT FUNCTION
# -------------------------
def chat_with_llm(message, history):
    messages = [{"role": "system", "content": "You are a professional financial advisor AI."}]

    for user, bot in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": bot})

    messages.append({"role": "user", "content": message})

    result = llm.chat.completions.create(
        model="meta-llama/Llama-3.2-3B-Instruct",
        messages=messages,
        max_tokens=500,
    )

    return result.choices[0].message["content"]


# -------------------------
# SPEECH TO TEXT
# -------------------------
def transcribe(audio):
    if audio is None:
        return ""
    output = whisper.audio.transcriptions.create(
        model="openai/whisper-large-v3",
        file=audio
    )
    return output.text


# -------------------------
# TEXT TO SPEECH
# -------------------------
def speak(text):
    audio_data = tts.audio.speech.create(
        model="facebook/fastspeech2-en-ljspeech",
        input=text
    )
    out_path = "/content/assistant_reply.wav"
    with open(out_path, "wb") as f:
        f.write(audio_data)
    return out_path


# -------------------------
# GRADIO APP
# -------------------------
with gr.Blocks() as demo:

    gr.Markdown("<h2 style='text-align:center;'>üíº FicalMind ‚Äì AI Financial Assistant (Voice + Text)</h2>")

    chatbot = gr.Chatbot(height=350)

    with gr.Row():
        user_text = gr.Textbox(label="Type your question")
        send_btn = gr.Button("Send")

    with gr.Row():
        audio_input = gr.Audio(sources=["microphone"], type="filepath", label="üé§ Speak")
        voice_btn = gr.Button("Speak & Ask")

    bot_voice_output = gr.Audio(label="üîä Assistant Voice Response")

    # -------- Text Input --------
    def add_user_text(msg, history):
        history.append((msg, None))
        return "", history

    def bot_reply(history):
        user_msg = history[-1][0]
        bot_msg = chat_with_llm(user_msg, history[:-1])
        history[-1] = (user_msg, bot_msg)

        voice = speak(bot_msg)
        return history, voice

    send_btn.click(add_user_text, [user_text, chatbot], [user_text, chatbot]).then(
        bot_reply, chatbot, [chatbot, bot_voice_output]
    )

    # -------- Voice Input --------
    def add_user_voice(audio, history):
        msg = transcribe(audio)
        history.append((msg, None))
        return "", history

    voice_btn.click(add_user_voice, [audio_input, chatbot], [user_text, chatbot]).then(
        bot_reply, chatbot, [chatbot, bot_voice_output]
    )

demo.launch(debug=True)

!pip install gradio==4.44.0 huggingface_hub pydub soundfile

import gradio as gr
from huggingface_hub import InferenceClient
import soundfile as sf
import os
import uuid

# ---------------------------------------------------------
# 1. SET HF TOKEN (Fixes your earlier ValueError)
# ---------------------------------------------------------
HF_TOKEN = "your_hf_token_here"   # <<< PUT YOUR TOKEN HERE
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HF_TOKEN

# ---------------------------------------------------------
# 2. Create HF clients
# ---------------------------------------------------------
asr_client = InferenceClient("openai/whisper-small", token=HF_TOKEN)   # Speech ‚Üí Text
llm_client = InferenceClient("mistralai/Mistral-7B-Instruct-v0.3", token=HF_TOKEN) # AI reply
tts_client = InferenceClient("facebook/fastspeech2-en-ljspeech", token=HF_TOKEN)    # Text ‚Üí Speech

# ---------------------------------------------------------
# 3. MAIN CHATBOT LOGIC
# ---------------------------------------------------------
def ficalmind_chat(audio, text):

    # CASE 1: User speaks
    if audio is not None:
        audio_path = f"input_{uuid.uuid4()}.wav"
        sf.write(audio_path, audio[1], audio[0])   # save audio input

        with open(audio_path, "rb") as f:
            text_input = asr_client.audio_to_text(audio=f)

    # CASE 2: User types
    elif text:
        text_input = text

    else:
        return "Please type or speak something.", None

    # -----------------------------------------------------
    # 4. FICALMIND Response (YOUR FINANCIAL AI LOGIC HERE)
    # -----------------------------------------------------
    prompt = f"""
You are FicalMind, a highly professional multimodal financial advisor.
Functions you must perform:

1. Financial guidance (savings, taxes, investments)
2. Budget summaries
3. Spending insights
4. Demographic-aware communication (student vs professional)
5. Use simple clear language
6. Always be formal & professional

User said: {text_input}

Respond accordingly.
"""

    ai_text = llm_client.text_generation(prompt, max_new_tokens=180)

    # -----------------------------------------------------
    # 5. Convert AI reply ‚Üí Speech
    # -----------------------------------------------------
    audio_output = tts_client.text_to_audio(text=ai_text)
    output_path = f"reply_{uuid.uuid4()}.wav"

    with open(output_path, "wb") as f:
        f.write(audio_output)

    return ai_text, output_path


# ---------------------------------------------------------
# 6. GRADIO USER INTERFACE
# ---------------------------------------------------------
with gr.Blocks(theme=gr.themes.Soft()) as demo:

    gr.Markdown("""
    # üíº FicalMind ‚Äì AI Financial Advisor
    ### Multimodal | Voice-enabled | Professional | IBM-style Interface
    """)

    with gr.Row():
        audio_in = gr.Audio(sources=["microphone"], type="numpy", label="üé§ Speak")
        text_in = gr.Textbox(label="‚å®Ô∏è Type your message")

    with gr.Row():
        text_out = gr.Textbox(label="ü§ñ FicalMind Response")
        audio_out = gr.Audio(label="üîä Voice Reply")

    submit_btn = gr.Button("Submit")

    submit_btn.click(
        ficalmind_chat,
        inputs=[audio_in, text_in],
        outputs=[text_out, audio_out]
    )

demo.launch()

!pip install -q gradio transformers huggingface-hub accelerate sentencepiece

import gradio as gr
from huggingface_hub import InferenceClient

# --------------------------------------
# ENTER YOUR HUGGINGFACE TOKEN HERE
# --------------------------------------
HF_TOKEN = "PASTE_YOUR_TOKEN_HERE"
# --------------------------------------

if HF_TOKEN.strip() == "":
    raise ValueError("‚ùó Please paste your HuggingFace token in HF_TOKEN.")

# LLM Client (Chat Model)
llm = InferenceClient(
    "meta-llama/Llama-3.2-3B-Instruct",
    token=HF_TOKEN
)

# Whisper STT (Speech-to-Text)
whisper = InferenceClient(
    "openai/whisper-large-v3",
    token=HF_TOKEN
)

# Text-to-Speech Model
tts = InferenceClient(
    "facebook/fastspeech2-en-ljspeech",
    token=HF_TOKEN
)

# -------------------------
# LLM TEXT CHAT FUNCTION
# -------------------------
def chat_with_llm(message, history):
    messages = [{"role": "system", "content": "You are a professional financial advisor AI."}]

    for user, bot in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": bot})

    messages.append({"role": "user", "content": message})

    result = llm.chat.completions.create(
        model="meta-llama/Llama-3.2-3B-Instruct",
        messages=messages,
        max_tokens=500,
    )

    return result.choices[0].message["content"]


# -------------------------
# SPEECH TO TEXT
# -------------------------
def transcribe(audio):
    if audio is None:
        return ""
    output = whisper.audio.transcriptions.create(
        model="openai/whisper-large-v3",
        file=audio
    )
    return output.text


# -------------------------
# TEXT TO SPEECH
# -------------------------
def speak(text):
    audio_data = tts.audio.speech.create(
        model="facebook/fastspeech2-en-ljspeech",
        input=text
    )
    out_path = "/content/assistant_reply.wav"
    with open(out_path, "wb") as f:
        f.write(audio_data)
    return out_path


# -------------------------
# GRADIO APP
# -------------------------
with gr.Blocks() as demo:

    gr.Markdown("<h2 style='text-align:center;'>üíº FicalMind ‚Äì AI Financial Assistant (Voice + Text)</h2>")

    chatbot = gr.Chatbot(height=350)

    with gr.Row():
        user_text = gr.Textbox(label="Type your question")
        send_btn = gr.Button("Send")

    with gr.Row():
        audio_input = gr.Audio(sources=["microphone"], type="filepath", label="üé§ Speak")
        voice_btn = gr.Button("Speak & Ask")

    bot_voice_output = gr.Audio(label="üîä Assistant Voice Response")

    # -------- Text Input --------
    def add_user_text(msg, history):
        history.append((msg, None))
        return "", history

    def bot_reply(history):
        user_msg = history[-1][0]
        bot_msg = chat_with_llm(user_msg, history[:-1])
        history[-1] = (user_msg, bot_msg)

        voice = speak(bot_msg)
        return history, voice

    send_btn.click(add_user_text, [user_text, chatbot], [user_text, chatbot]).then(
        bot_reply, chatbot, [chatbot, bot_voice_output]
    )

    # -------- Voice Input --------
    def add_user_voice(audio, history):
        msg = transcribe(audio)
        history.append((msg, None))
        return "", history

    voice_btn.click(add_user_voice, [audio_input, chatbot], [user_text, chatbot]).then(
        bot_reply, chatbot, [chatbot, bot_voice_output]
    )

demo.launch(debug=True)









