# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UqsKIrvGgAhGq_OXf0yWgt7NgpdEq1bx
"""

!pip install -q transformers accelerate safetensors huggingface-hub gradio googletrans==4.0.0-rc1

# -------------------- Step 0: Force compatible versions --------------------
!pip install -q numpy==1.26.4 httpx==0.28.1 websockets==13.0.0

# -------------------- Step 1: Install required packages --------------------
!pip install -q transformers accelerate safetensors huggingface-hub gradio googletrans==4.0.0-rc1

# -------------------- Step 2: Import libraries --------------------
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed
import gradio as gr
from googletrans import Translator

# -------------------- Step 3: Load IBM Granite 3.3 2B instruct --------------------
model_name = "ibm-granite/granite-3.3-2b-instruct"
device = "cuda" if torch.cuda.is_available() else "cpu"

print("Loading model... This may take a few minutes.")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16 if device=="cuda" else torch.float32
)
print("Model loaded successfully!")

translator = Translator()

# -------------------- Step 4: Prompt builder --------------------
def build_prompt(user_input, age, occupation, income):
    prompt = (
        f"You are FicalMind, a professional financial AI assistant.\n"
        f"User profile: age {age}, occupation {occupation}, monthly income {income} INR.\n"
        f"Provide detailed financial advice including savings, investments, budgeting, and spending insights.\n"
        f"Respond formally, professionally, and clearly.\n"
        f"User: {user_input}\nAssistant:"
    )
    return prompt

# -------------------- Step 5: Response generator --------------------
def generate_response(user_input, age, occupation, income, lang):
    prompt = build_prompt(user_input, age, occupation, income)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    set_seed(42)

    output_ids = model.generate(
        **inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )
    response = tokenizer.decode(output_ids[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)

    # Translate if needed
    if lang != "en":
        response = translator.translate(response, dest=lang).text
    return response

# -------------------- Step 6: Gradio UI --------------------
def chat_interface(user_input, age, occupation, income, lang):
    return generate_response(user_input, age, occupation, income, lang)

with gr.Blocks() as demo:
    gr.Markdown("# ðŸ’¬ FicalMind â€” Your Professional Financial AI Assistant")

    with gr.Row():
        with gr.Column(scale=0.7):
            user_input = gr.Textbox(label="Your Question", lines=3, placeholder="E.g., How much should I save monthly?")
            age = gr.Number(label="Your Age", value=25)
            occupation = gr.Textbox(label="Your Occupation", value="student / professional")
            income = gr.Number(label="Monthly Income (INR)", value=30000)
            lang = gr.Dropdown(label="Reply Language", choices=["en", "hi", "te"], value="en")
            submit = gr.Button("Ask FicalMind")
        with gr.Column(scale=1.3):
            output = gr.Textbox(label="FicalMind's Reply", lines=10)

    submit.click(fn=chat_interface, inputs=[user_input, age, occupation, income, lang], outputs=output)

demo.launch(share=True)

